#make a list
myList<-list(b,e,myMatrix2)
#make a list
myList<-list(d,e,myMatrix2)
#view the list
myList
#lists can be names for easier access
names(myList)<-c("g2", "g3","g4")
#lists can be accessed
myList[1:3]
ListOf5Variables<-list("j", "k", "l", "m", "n")
ListOf5Variables[2]
ListOf5Variables<-list("j", "k", "l", "m", "n")
ListOf5Variables[2]
ListOf5Variables[2:4]
ListOf5Variables[2*4]
ListOf5Variables<-list(1:5)
ListOf5Variables[2]
ListOf5Variables[2:4]
ListOf5Variables<-list("1", "2", "3", "4", "5"")
ListOf5Variables[2]
ListOf5Variables[2:4]
ListOf5Variables<-list("1", "2", "3", "4", "5")
ListOf5Variables[2]
ListOf5Variables[2:4]
ListOf5Variables[2*4]
ListOf5Variables[2]*ListOf5Variables[4]
ListOf5Variables[[2]]
ListOf5Variables[[2]]*ListOf5Variables[[4]]
names(ListOf5Variables)<-c("variable1", "variable2", "variable3", "variable4", "variable5")
names(ListOf5Variables)<-c("variable1", "variable2", "variable3", "variable4", "variable5")
ListOf5Variables[variable1]
names(ListOf5Variables)<-c("variable1", "variable2", "variable3", "variable4", "variable5")
ListOf5Variables["variable1"]
for(counter in 1:3){
print(myMatrix2[counter,])
}
for(counter in 1:3){
print(myMatrix2[counter,])
}
myMatrix2
for(counter in 1:3){
print(mean(myMatrix2[ ,counter]))
}
for(counter 1:3) {
for(counter in 1:3) {
matrixFromAVector<-cbind(d)
}
matrixFromAVector
for(counter in 1:3) {
print(d)
}
matrixFromAVector
for(counter in 1:3) {
cbind(print(d))<-matrixFromAVector2
}
for(counter in 1:3) {
cbind(print(d))
}
matrixFromAVector
for(counter in 1:3) {
cbind(print(d))
}
for(counter in 1:3) {
z<-cbind(print(d))
}
z
z<-cbind(d, d, d)
z
for(counter in 1:3){
z<-cbind(d, d, d)
}
z
for(counter in 1:3){
z<-cbind(d)
}
z
for(counter in 1:3){
z<-cbind(d, d, d)
}
z
test=1
while(test < 6){
#do something...
print(test)
test=test+1
}
while(d<3){
print(d)
d=d+1
}
while(d<8){
print(d)
d=d+1
}
read.csv("myDataFile.csv")
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data"
data <- read.table(url, sep=",")
dim(data)
data(mtcars)
head(mtcars)
summary(mtcars)
head(data)
summary(data)
colnames(data)<-c("class", "age", "menopause", "tumorSize", "invNodes", "nodeCaps", "degMalig", "breast", "breastQuad", "irradiate")
head(data)
head(data$class)
levels(data$class)
#The split funciton can divide the data based on the category
dataSplit<-(split.data.frame(data,data$class))
summary(dataSplit[[1]])
summary(dataSplit[[2]])
while(d<11){
print(d)
d=d+1
}
d=1
while(d < 11){
print(d)
d=d+1
}
d=[8:11]
d->c(8:11)
d <- c(1,2,3,4)
e <- c("red", "white", "red", NA)
f <- c(TRUE,TRUE,TRUE,FALSE)
mydata <- data.frame(d,e,f)
names(mydata) <- c("myNumber","myWord","TorF") # variable names
mydata
print(d[1])
print(d[1])->c
c
while(c < 3){
print(d[1])->c
c=c+1
}
d[1+1]
while(d[]<3) {
print(d)
d=d+1
}
read.csv("myDataFile.csv")
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data"
data <- read.table(url, sep=",")
dim(data)
colnames(data)<-c("class", "age", "menopause", "tumorSize", "invNodes", "nodeCaps", "degMalig", "breast", "breastQuad", "irradiate")
head(data)
head(data$class)
#The split funciton can divide the data based on the category
dataSplit<-(split.data.frame(data,data$class))
#The split funciton can divide the data based on the category
dataSplit<-(split.data.frame(data,data$class))
summary(dataSplit[[1]])
summary(dataSplit[[2]])
dataSplit<-split.data.frame(data,data$irradiate)
summary(dataSplit[[1]])
summary(dataSplit[[2]])
url <- "https://archive.ics.uci.edu/ml/datasets.html""
url <- "https://archive.ics.uci.edu/ml/datasets.html""
url <- "https://archive.ics.uci.edu/ml/datasets.html"
data2 <- read.table(url, sep=",")
url <- "https://archive.ics.uci.edu/ml/datasets.html"
data2 <- read.table(url, sep=",")
dim(data2)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data"
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data"
arrhytmiaData <- read.table(url, sep=",")
dim(data2)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.data"
arrhytmiaData <- read.table(url, sep=",")
dim(arrhytmiaData)
colnames(balloonData)<-c("color", "size", "act", "age", "inflated")
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data"
balloonData <- read.table(url, sep=",")
dim(balloonData)
colnames(balloonData)<-c("color", "size", "act", "age", "inflated")
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data"
balloonData <- read.table(url, sep=",")
dim(balloonData)
colnames(balloonData)<-c("color", "size", "act", "age", "inflated")
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data"
balloonData <- read.table(url, sep=",")
dim(balloonData)
colnames(balloonData)<-c("color", "size", "act", "age", "inflated")
head(balloonData)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data"
balloonData <- read.table(url, sep=",")
dim(balloonData)
colnames(balloonData)<-c("color", "size", "act", "age", "inflated")
head(balloonData)
balloonDataSplit<-split.data.frame(balloonData,balloonData$age)
summary(balloonDataSplit[[1]])
summary(balloonDataSplit[[2]])
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/yellow-small.data"
balloonData <- read.table(url, sep=",")
dim(balloonData)
colnames(balloonData)<-c("color", "size", "act", "age", "inflated")
head(balloonData)
balloonDataSplit<-split.data.frame(balloonData,balloonData$size)
summary(balloonDataSplit[[1]])
summary(balloonDataSplit[[2]])
help(runif)
runif()
x<-1:4
lapply(x, runif)
help(na.rm)
library(datasets)
data("iris")
?iris
apply(iris, 1, mean)
warnings()
apply(iris$Sepal.Length, 1, mean)
head(iris)
iris
f<-gl(3,50)
tapply(iris$Sepal.Length, f, mean)
apply(iris,2,mean)
colMeans(iris)
iris
apply(iris,1,mean)
rowMeans(iris[,1:4])
apply(iris[,1:4],1,mean)
apply(iris[,1:4],2,mean)
library(datasets)
data(mtcars)
?mtcars
apply(mtcars,2,mean)
sapply(mtcars(split(mtcars$mpg,mtcars$cyl),mean))
sapply(split(mtcars$mpg,mtcars$cyl),mean)
sapply(mtcars$mpg,mtcars$cyl,mean)
mtcars$cyl
split(mtcars,mtcars$cyl)
mean(mtcars$mpg,mtcars$cyl)
tapply(mtcars$cyl,mtcars$mpg,mean)
sapply(mtcars,cyl,mean)
lapply(mtcars,mean)
with(mtcars,tapply(mpg,cyl,mean))
tapply(mtcars$mpg,mtcars$cyl,mean)
debug(1s)
help(is)
debug(Is)
debug(is)
is()
``
library('xlsx')
fun(xlsx, xlsx)
require(xlsx)
library("rJava", lib.loc="~/R/win-library/3.6")
remove.packages("rJava", lib="~/R/win-library/3.6")
install.packages("rJava")
install.packages('RMySQL',type='source')
mysql --user-genome --host-genome-mysql.cse.ucsc.edu -A
ucscDb<-dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
library('RMySQL')
ucscDb<-dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
result<-dbGetQuery(ucscDb, "show databases;")
result
dbDisconnect(ucscDb)
result
hg19<-dbConnect(MySQL(), user="genome", db="hg19", host="genome-mysql.cse.ucsc.edu")
allTables<-dbListTables(hg19)
length(allTables)
allTables[1:5]
dbListField(hg19, "affyU133Plus2")
dbListFields(hg19, "affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
affyData<-dbReadTable(hg19, "affyU133Plus2")
head(affyU133Plus2)
head(affyData)
query<-dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis<-fetch(query)
quantile(affyMis$misMatches)
affyMisSmall<-fetch(query,n=10)
affyMisSmall
dbClearResult(query)
dim(affyMisSmall)
dbDisconnect(hg19)
source("http://bipoconductor.org/biocLite.R")
source("http://bioconductor.org/biocLite.R")
source("https://bioconductor.org/biocLite.R")
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install()
biocLite('rhdf5')
library(BiocManager)
biocLite('rhdf5')
biocLite("rhdf5")
BiocManager::available()
BiocManager::install(c("rhdf5"))
library(rhdf5)
created=h5createFile("example.h5")
created
created=h5createGroup("example.h5", "foo")
created=h5createGroup("example.h5", "baa")
created=h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
library(httr)
install.packages("httr")
library(httr)
install.packages("jsonLite")
y
install.packages("jsonlite")
library(jsonlite)
install.packages("httpuv")
library(httpuv)
oauth_endpoints("github")
myapp<-oauth_app(appname="Andrada_App", key="340369275041c3af69a2", secret="9e00f9a669b18ccd2824a396ae22fe1f4cc93179")
github_token<-oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken<-config(token=github_token)
req<-GET("https://api.github.com/users/jtleek/repos", gtoken)
json1=content(req'')
json1=content(req)
json2=jsonlite::fromJSON(tojson(json1))
json2=jsonlite::fromJSON(toJSON(json1))
json2[1,1"4"]
json2[1,1:4]
json2[1,1:8]
json2[1,"created_at"]
json2[,"created_at"]
json2[json2$full_name=="jtleek/datasharing", "created_at"]
install.paclages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
remove.packages("RMySQL")
remove.packages("DBI")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
help(read.fwf)
url<-https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for
url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
url<-"http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for"
read.fwf(url, widths = 9, header=TRUE, sep=" ")
read.fwf(url, widths = 4, header=TRUE, sep=" ")
data<-read.fwf(url, widths = 9, header=TRUE, sep=" ")
data<-read.fwf(url, widths = 9, header=TRUE, skip=2)
data
data<-read.fwf(url, widths = 13, header=TRUE, skip=2)
data
data<-read.fwf(url, widths = c(-58,3), header=TRUE, skip=2)
data
data<-read.fwf(url, widths = c(-59,3), header=TRUE, skip=2)
data
data<-c(read.fwf(url, widths = c(-59,3), header=TRUE, skip=2))
is.vector(data)
sum(data)
data<-sum(read.fwf(url, widths = c(-59,3), header=TRUE, skip=2))
data<-sum(read.fwf(url, widths = c(-59,3), header=TRUE, skip=2))
data<-read.fwf(url, widths = c(-59,3), header=TRUE, skip=2)
header(data)
head(data)
data<-read.fwf(url, widths = c(-59,3), header=TRUE, skip=4)
head(data)
data<-read.fwf(url, widths = c(-59,3), header=TRUE, skip=2)
head(data)
data<-read.fwf(url, header=TRUE, skip=2)
data<-read.fwf(url, widths=62, header=TRUE, skip=2)
head(data)
is.data.frame(data)
data<-read.fwf(url, widths=c(-28,4), header=TRUE, skip=2)
head(data)
data<-read.fwf(url, widths=c(-28,4), header=TRUE, skip=4)
head(data)
data<-read.fwf(url, widths=c(-28,4), header=TRUE, skip=3)
head(data)
sum(data[,4])
data<-read.fwf(url, widths=c(-1,9,-5,4,-1,3,-5,4,-1,3,-5,4,-1,3,-5,4,-1,3), header=TRUE, skip=3)
data<-read.fwf(url, widths=c(-1,9,-5,4,-1,3,-5,4,-1,3,-5,4,-1,3,-5,4,-1,3), header=TRUE, skip=4)
data<-read.fwf(url, widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), header=TRUE, skip=4)
data<-read.fwf(url, widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
head(data)
sum(data[,4])
data
data[,4]
get<-c(data[,4])
sum(get)
url<-"http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for"
data<-read.fwf(url, widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
head(data)
sum(data[,4])
data<-read.fwf(file=url, widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
sum(data[,4])
help(download.file)
url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
url
getwd()
download.file(url, destfile="./R/Coursera_Data_Science_Specialization/GettingAndCleaningData")
download.file(url, destfile="./R/Coursera_Data_Science_Specialization/GettingAndCleaningData/acs.csv", method="curl")
acs<-download.file(url, destfile="./R/Coursera_Data_Science_Specialization/GettingAndCleaningData/acs.csv", method="curl")
head(acs)
is.data.frame(acs)
is.list(acs)
help(read.csv)
acsdata<-read.csv(csv, header=TRUE, sep=",")
acsdata<-read.csv(acs, header=TRUE, sep=",")
acs
acsdata=read.csv("./R/Coursera_Data_Science_Specialization/GettingAndCleaningData/acs.csv")
head(acsdata)
is.data.frame(acsdata)
acsdata[(ACR>10 & AGS==6),]
acsdata=read.csv("./R/Coursera_Data_Science_Specialization/GettingAndCleaningData/acs.csv", col.names= "RT", "SERIALNO", "DIVISION", "PUMA", "REGION", "ST",  "ADJUST", "WGTP", "NP", "TYPE", "ACR", "AGS", "BDS", "BLD", "BUS", "CONP", "ELEP", "FS")
colnames(acsdata)
acsdata[("ACR">10 & "AGS"==6),]
acsdata=read.csv("./R/Coursera_Data_Science_Specialization/GettingAndCleaningData/acs.csv")
acsdata[("ACR">10 & "AGS"==6),]
acsdata=read.csv("./R/Coursera_Data_Science_Specialization/GettingAndCleaningData/acs.csv")
head(acsdata)
acsdata[(acsdata$ACR>10 & acsdata$AGS==6),]
acsdata[(acsdata$ACR>10),]
acsdata[(acsdata$ACR>=3),]
acsdata[(acsdata$ACR>=1),]
acsdata[which(acsdata$ACR>=1),]
acsdata[which(acsdata$ACR>=3),]
acsdata[which(acsdata$ACR>=3 & acsdata$AGS==6),]
agricultureLogical<-acsdata[which(acsdata$ACR>=3 & acsdata$AGS==6),]
which(agricultureLogical)
acsLogical=ifelse((\acsdata$ACR>=3 & acsdata$AGS==6), TRUE, FALSE)
acsLogical=ifelse((acsdata$ACR>=3 & acsdata$AGS==6), TRUE, FALSE)
acsLogical
which(acsLogical)
getwd()
setwd("./R/Coursera_Data_Science_Specialization/GettingAndCleaningData")
url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
acsidaho<-read.csv(url)
head(acsidaho)
help(strsplit)
names(acsidaho)
strsplit(names(acsidaho), string="wgtp")
strsplit(names(acsidaho), "wgtp")
url2<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
help("read.csv")
gdp<-read.csv(url2, header=TRUE, sep=",", skip=4, nrows=190)
head(gdp)
commasgone<-sub(",", " ", gdp$X.4)
head(commasgone)
commasgone<-sub(",", " ", gdp$X.4, )
head(commasgone)
commasgone<-gsub(",", " ", gdp$X.4)
head(commasgone)
average(gdp$X.4)
mean(gdp$X.4)
commasgone<-gdp[,(gsub(",", " ", gdp$X.4))]
as.vector(commasgone)
as.numeric(commasgone)
commasgone<-gsub(",", " ", gdp$X.4)
head(commasgone)
commasgone<-as.numeric(gsub(",", " ", gdp$X.4))
commasgone<-gsub(",", " ", gdp$X.4)
is.character((commasgone))
commasgone<-gsub(",", "", gdp$X.4)
head(commasgone)
commasgone<-as.numeric(gsub(",", "", gdp$X.4))
head(commasgone)
mean(commasgone)
head(gdp)
grep("^United, gdp$X.3"),3
country<-grep("^United, gdp$X.3"),3
country<-grep("^United, gdp$X.3")
grep("^United", gdp$X.3),3
grep("^United", gdp$X.3)
grep("*United", gdp$X.3)
grep("United$", gdp$X.3)
url3<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
head(gdp)
url4<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
edu<-read.csv(url4, header=TRUE, sep=",")
head(edu)
mergedData<-merge(gdp, edu, by.x = "X", by.y="CountryCode")
head(mergedData)
names(mergedData)
grep("Fiscal year end$", mergedData$Special.Notes)
grep("Fiscal year end$", mergedData$Special.Notes, value=TRUE)
grep("Fiscal year end", mergedData$Special.Notes, value=TRUE)
FYE<-mergedData[grep("Fiscal year end", mergedData$Special.Notes), ]
head(FYE)
FYE<-mergedData[grep("Fiscal year end: June", mergedData$Special.Notes), ]
head(FYE)
nrow(FYE)
install.packages("quantmod")
library(quantmod)
amzn=getSymbols("AMZN", auto.assign=FALSE)
sampleTimes=index(amzn)
head(sample\)
head(sampleTimes)
sampleTimes
grep("2012", sampleTimes)
length(grep("^2012", sampleTimes))
format(grep("2012", sampleTimes), %a)
format(grep("2012", sampleTimes), %a, %b, %d)
date2012<-grep("2012", sampleTimes)
format(date2012, %a, %b, %d)
getwd()
setwd(./week4/UCI_Har_Dataset)
setwd("./week4/UCI_Har_Dataset")
getwd()
setwd("C:/Users/andra/Documents/R/Coursera_Data_Science_Specialization/GettingAndCleaningData/week4/UCI_Har_Dataset")
help("read.csv")
subject_test<-read.table(subject_test)
subject_test<-read.table(subject_test.txt)
setwd("./test")
subject_test<-read.table(subject_test.txt)
getwd()
subjectTest<-read.table(subject_test.txt)
subjectTest<-read.table("./test/subject_test.txt")
setwd("C:\Users\andra\Documents\R\Coursera_Data_Science_Specialization\GettingAndCleaningData\week4\UCI_HAR_Dataset\test")
setwd(".\Documents\R\Coursera_Data_Science_Specialization\GettingAndCleaningData\week4\UCI_HAR_Dataset\test")
setwd("Documents\R\Coursera_Data_Science_Specialization\GettingAndCleaningData\week4\UCI_HAR_Dataset\test")
